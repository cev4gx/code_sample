Optimizations

The first optimization that I worked on was working on a different hash function. At first I used the sum of each letter’s ASCII value multiplied by 36^i where i was the position of the letter in the string. I figured that the pow() function was pretty slow, and I noticed that I was getting 0 was the hash value a lot, and it seemed to be popping up more than any other number. I am still not sure whether that was actually happening or if my brain picked out the 0’s more than any other number. A new hash function that I tried was still the sum of each of the letter’s ASCII values, but each multiplied by 37 instead of 37^i. Finally, I multiplied the sum by the total length of the word. This sped the wordPuzzle up slightly, but not much a huge difference. Even though there was not a huge difference I still kept the new hash function since it was slightly simpler and still got the job done.

The next optimization was implementing a prefix hash table, in which I added the first three letters of each word to a hash table, if it was not already there. I generated this table at the same time as the dictionary hash table was created, and then in the quad-nested for loop, if the first three letters from the returned value of getWordInGrid() were not contained in the prefix table, then I broke and moved on to the next direction. This cut down the time of the 300x300 grid with words2.txt to ~32 seconds, as opposed to ~1060 seconds from the original code. So this produced a speedup of ~33.

As I inspected what my hash table of words actually looked like (where chunks of words were and where lots of space was), I decided to cut the size by half just to see what would happen. This did increase my load factor to ~1, but the size of the table would decrease by half, which I thought could be a good thing. So, instead of multiplying the number of words by 2 when generating the size of my hash table, I just got the next prime from the number of words. This did in fact cut the time – and cut it more than I was expecting. The time reduced by almost half, to ~17 seconds when run on the 300x300 grid with words2.txt. This seemed odd, since a smaller load factor is supposed to help with hash tables, so I began inspecting why this cut down the time the way it did.

I ended up finding that in my contains() method for my hash table, I was circling the entirety of the table if the group of letters was not contained in the hash table. The condition for my while loop to increment the index for linear probing in contains was simply if the word at the index was not equal to the current group of letters. Since I used linear probing, if the word was not at the original hash value then if it was in the hash table, you would reach the word before an empty cell (containing an empty string). Because of this logic, I changed my while loop condition to (word at index != group of letters OR word at index != “”). The time after this change was ~24 seconds, which was slightly higher than the last optimization of decreasing the hash table size.

With this, I decided to see what would happen if I changed the load factor back to ~0.5 by getting the next prime of the number of words times 2. With this change (back to what it was originally), the time was cut down to ~1.65 seconds.

In my pre-lab files, my for loop for length of the word was from 3 to the max(rows, columns), which I realized was somewhat infeasible for a word to be that long, so I changed the length to be a max of the constant 23 (as suggested by a TA). This cut down the time to ~0.237 seconds. Finally, instead of printing as I went inside the for loop, I stored the outputs in a vector of strings and then went through this vector with a for loop to print each output, after the timer had stopped. This was my final optimization for the post-lab, and the final time for the 300x300 grid with words2.txt was ~0.234 seconds. The total speedup after all of these optimizations was 4,531.
